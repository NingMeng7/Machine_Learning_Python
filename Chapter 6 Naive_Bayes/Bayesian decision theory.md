## 贝叶斯分类器
Bayesian decision theory 是概率框架下实施决策的基本方法，对分类任务而言，在所有相关概率已知的理想情况下，贝叶斯决策论考虑基于这些概率和误判损失来选择最优的类别标记。

## 1. 贝叶斯分类器的学习方法
假设有N种可能的类别标记 $\mathcal{Y} = \{c_1,c_2,...,c_N\}$, $J_{ij}$是将一个真实标记$c_j$误分类成$c_i$所产生的期望损失,则在样本x上,误分类为$c_i$的条件期望损失是:
$$R(c_i|x) = \sum_{j=1}^N J_{ij}P(c_j|x)$$

我们希望找到一个$h: \mathcal{X} \to \mathcal{Y}$来最小化总体风险:
$$R(h) = E_x[R(h(x)|x)]$$
为此，我们考虑对所有的样本x，最小化其条件风险$R(h(x)|x)$,即，我们的判定准则(策略)是:
$$h*(x) = arg min_{c\in \mathcal{Y}} R(c|x)$$

如果我们选择将损失函数定义为0/1损失，也就是最小化分类错误率，那么有:
$$R(c|x) = 1 - P(c|x)$$
为了让条件风险最小，我们把问题转化成，对每个样本:
$$h*(x) = arg max_{c \in \mathcal{Y}} P(c|x)$$
**我们对每个样本，选择后验概率最大的那个类别标记！**

## 2. 如何获得P(c|x)
我们需要做的，是在有限的训练样本上，通过样本对我们假设存在的一个统计分布的各种概率进行一个估计,有:
$$P(c|x) = \frac{P(x,c)}{P(x)} = \frac{P(c)P(x|c)}{P(x)}$$
为了对不同的$c_i \in C$进行决策，我们实际上只需要对分子进行考虑即可，其中P(c)是类的一个先验概率，当训练集包含足够多的i.i.d.样本的时候，我们可以通过大数定律来通过频率对P(c)进行估计.而类条件概率P(x|c)涉及到了x所有属性的联合概率，例如，d个二值属性的样本空间将会有$2^d$种取值,这通常比m要大很多，很多样本取值将不会在训练集中出现，直接用频率估计导致大量的估计概率0出现，显然这是有问题的。

## 2.1 极大似然估计
为了估计出P(x|c),一种常用的策略是先假定其具有某种确定的概率分布形式，再基于训练样本对概率分布的参数进行估计(极大似然估计).具体地说，我们假设P(x|c)具有确定地形式并且被参数向量$\theta_c$唯一确定.

我们采用频率主义学派地极大似然估计(Maximum Likelihood Estimation), 令$D_c$表示训练集D中第c类样本组成地集合,假设这些样本是独立同分布的，则:
$$P(D_c|\theta_c) = \prod_{x \in D_c} P(x|\theta_c)$$
使得这个概率最大的那个参数就是我们要找的参数。

为了避免下溢出，我们采用对数似然函数:
$$LL(\theta_c) = log P(D_c|\theta_c) = \sum_{x \in D_c} log P(x|\theta_c)$$

最后有:
$$\theta_c = argmax LL(\theta_c)$$

## 3.1 朴素贝叶斯分类器
为了解决我们之前提到的问题,也就是我们的训练集m通常比指数级的样本空间容量小得多，具体地讲，假设文本分类中我们有 50000 个词，那么$x \in \{0, 1\}^{50000}$，为了估计$P(x|\theta_c)$，我们不得不考虑考虑$2^{50000}$结果，从而需要$2^{50000} - 1$个参数(概率的归一化减少了一个). 为此，我们做一个很强的假设，对已知的类别，假设所有的属性相互独立，也就是各个属性独立地对分类结果产生影响:
$$P(c|x) = \frac{P(c)P(x|c)}{P(x)} = \frac{P(c)}{P(x)}P(x_1|c)P(x_2|c,x_1)P(x_3|c,x_1,x_2)...P(x_d|c,x_1,x_2,....,x_{d-1})$$
$$= \frac{P(c)}{P(x)}P(x_1|c)P(x_2|c)...P(x_d|c)$$

这样,我们就得到了朴素贝叶斯的判定准则:
$$h_{nb}(x) = argmax_{c \in \mathcal{Y}} P(c) \prod_{i=1}^d P(x_i|c)$$

为了给出判定，我们需要从训练集中得到先验概率P(c)和每个属性的条件概率$P(x_i|c)$.
$$P(c) = \frac{|D_c|}{|D|}$$
$$P(x_i|c) = \frac{D_{c,x_i}}{|D_c|}$$

上面这两个式子还有一个问题，连乘有一个问题是，只要一个元素为0，全部gg，而由于样本本身的缺陷，我们可能会面临一个情况就是某种特征取值没有出现在这个样本中，而样本没有观测到并不能完全说明这个概率是0，因此，我们要对这两个式子进行平滑处理,用N表示训练集中的可能类别数，$N_i$表示第i个属性的可能取值数:
$$P(c) = \frac{|D_c|+1}{|D|+N}$$
$$P(x_i|c) = \frac{D_{c,x_i}+1}{|D_c|+N_i}$$

## 3.2 朴素贝叶斯分类器的算法
(1) 计算先验概率P(c) 以及条件概率$P(x_i|c)$
$$P(c) = \frac{|D_c|+1}{|D|+N}$$
$$P(x_i|c) = \frac{D_{c,x_i}+1}{|D_c|+N_i}$$

2. 对于给定的实例x，计算:

$$P(Y=c_k)\prod_{j=1}^d P(x|Y=c_k)$$

3. 根据决策策略:
$$y = argmax_{c_k} P(Y=c_k) \prod_{j=1}^d P(x|Y = c_k)$$
