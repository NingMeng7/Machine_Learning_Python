## 1.1 决策树概念
决策树(decision tree)是一种基本的分类和回归的方法，这里主要讨论用于分类的决策树。在分类问题中，决策树表示基于特征对实例进行分类的过程，可以认为是 if-then规则的集合,也可以认为是定义在特征空间和类空间上的条件概率分布.
$$f: \mathcal{X} \to \mathcal{Y}$$

通常，决策树学习需要经过3个步骤:

1. 特征选择
2. 决策树的生成
3. 决策树的修剪(防止过拟合)

*Definition:* 分类决策树模型是一种描述对实例进行分类的树形结构，决策树由结点(Node) 与 有向边(directed edge) 组成. 结点分为内部结点(internal node) 和 叶结点(leaf node). 内部结点表示一个feature,叶结点表示一个类。

## 1.2 决策树 与 if-then
当使用决策树进行分类的时候，从根结点开始，对实例的某一个特征进行测试,根据测试的结果，将实例分配到子节点向下走，每一个子结点都对应这这个特征的一个取值，递归直到到达叶结点。

决策树的路径或其所对应的if-then规则集合具有一个重要的性质: 互斥且完备，即,每个实例有且仅有一条路径对应。

问题: 如果测试数据集的某个特征出现了训练数据集中没有出现的取值怎么办。

## 2 决策树学习准备
决策树学习的本质上是从训练数据集中归纳出一组分类的规则, 与训练数据集不相矛盾的决策树(完全正确分类训练集)可能有很多，也可能没有。我们需要的是与训练数据集矛盾较小的决策树，注意，我们不仅仅希望我们的模型在训练集上的效果很好，还希望它对未知数据有很好的泛化能力。

## 2.1 特征选择
对一个样本来说，可能会有很多的特征，不同的特征对于我们决策的帮助也不同，这就涉及到一个特征选择的问题。如果根据一个特征进行分类的结果与随机分类的结果没有很大差别，则称这个特征是没有分类能力的，经验上，去除这样的特征对决策树的学习影响不大。

**特征选择的准则**: 信息增益/信息增益比

### 2.1.1 熵
**熵(entropy)**: 是表示随机变量不确定性的度量，假设x是一个取值空间容量有限的离散随机变量:
$$P(X=x_i) = p_i$$
则随机变量X的熵为:
$$H(X) = - \sum_{i=1}^n p_ilogp_i$$

信息熵有几个基本的性质:

1. 信息熵只依赖于X的分布，和X的取值无关
2. 熵越大，随机变量的不确定性就越大，考虑极端的情况 $0log0 = 0$ $1log1 = 0$
3. $0 \leq H(p) \leq log n$

### 2.1.2 条件熵 
**条件熵**: H(Y|X) 表示在已知随机变量X的条件下随机变量Y的不确定性,对$i=1,2,3,...n$, $j=1,2,3,...,m$
$$H(Y|X) = \sum_{i=1}^n p_iH(Y|X = x_i)$$
其中, $p_i = P(X = x_i), i=1,2,...,n$ 

### 2.1.3 经验熵与经验条件熵
当熵和条件熵中的概率(注意，我们虽然假设了分布的存在，但是这个分布实际是不可知的)，由估计得到(例如，极大似然估计)，我们分别称这时候的熵是经验熵和经验条件熵(empirical (conditional) entropy)

### 2.1.4 信息增益
**信息增益**: 特征A对训练数据集D的信息增益g(D,A)定义为集合D的经验熵H(D)与特征A给定条件下D的经验条件熵H(D|A)之差:(熵减)
$$g(D,A) = H(D) - H(D|A)$$


## 2.2 信息增益算法
假设对训练数据集D, |D|表示其样本容量，设有K个类$C_k, k = 1,2,...,K$, $|C_k|$是属于K类的样本个数，显然有$\sum_{k=1}^K|C_k| = |D|$. 

设特征A有n个不同的取值$\{a_1,a_2,...,a_n\}$,根据特征A的取值划分为n个子集,$D_1,D_2,...,D_n$,则有:$\sum_{i=1}^n|D_i| = |D|$

再设子集$D_i$中属于类k的样本集合为$D_{ik}$，其样本容量为$|D_{ik}|$,最终有:

*Algorithm*: Calculate information gain

input: training set D and feature A

output: information gain related to D & A: g(D,A)

- 1. 计算数据集D的经验熵$H(D) = -\sum^K_{k=1} \frac{|C_k|}{|D|}log_2 \frac{|C_k|}{|D|}$
- 2. 计算特征A对数据集D的经验条件熵$H(D|A) = -\sum_{i=1}^n \frac{|D_i|}{|D|}H(D_i) = -\sum_{i=1}^n \frac{|D_i|}{|D|}\sum_{k=1}^K \frac{|D_{ik}|}{|D_i|}log_2 \frac{|D_{ik}|}{|D_i|}$
- 3. g(D,A) = H(D) - H(D|A)

## 2.3 信息增益比
用信息增益作为划分训练数据集的特征，有一个问题是，偏向于选择取值较多的特征。为了缓解这个问题，引入信息增益比(information gain ratio).

**信息增益比**
特征A对训练数据集D的信息增益比$g_R(D,A)$定义为:
$$g_R(D,A) = \frac{g(D,A)}{H_A(D)}$$
其中$H_A(D) = - \sum_{i=1}^n \frac{|D_i|}{|D|} log_2 \frac{|D_i|}{|D|}$

## 3 决策树的生成算法
这部分介绍两种决策树的生成算法

## 3.1 ID3 Algorithm
input： training set D, feature set A, threshold $\epsilon$

output：Decision Tree T

- 1. 如果D中的所有instances 属于同一个类 $C_k$,则T为以$C_k$为类标记的单结点树，**返回T.**
- 2. 如果A为空集，即所有的可供决策的特征已经使用完全，则T为单结点树，使用多数表决的方式决定$C_k$类标记，**返回T**.
- 3. 否则，计算特征A中各个特征对D的信息增益，选择信息增益最大的特征$A_g$.
- 4. 如果$A_g$的信息增益小于$\epsilon$,则T为单结点树，使用多数表决的方式决定$C_k$类标记,**返回T**.
- 5. 否则，对$A_g$每一个可能的特征取值$a_i$分割数据集D，将$D_i$中实例数最大的类作为标记，构建子结点，由结点和子结点构成树T，返回T
- 6. 对第i个子结点,以$D_i$为训练集,$A - \{A_g\}$为特征集合，递归建树，得到子树$T_i$，返回$T_i$.

## 3.2 C4.5 Algorithm
将第三步的信息增益改换为信息增益比。

 
