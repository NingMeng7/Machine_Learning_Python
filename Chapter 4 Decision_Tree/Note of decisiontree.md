## 1.1 决策树概念
决策树(decision tree)是一种基本的分类和回归的方法，这里主要讨论用于分类的决策树。在分类问题中，决策树表示基于特征对实例进行分类的过程，可以认为是 if-then规则的集合,也可以认为是定义在特征空间和类空间上的条件概率分布.
$$f: \mathcal{X} \to \mathcal{Y}$$

通常，决策树学习需要经过3个步骤:

1. 特征选择
2. 决策树的生成
3. 决策树的修剪(防止过拟合)

*Definition:* 分类决策树模型是一种描述对实例进行分类的树形结构，决策树由结点(Node) 与 有向边(directed edge) 组成. 结点分为内部结点(internal node) 和 叶结点(leaf node). 内部结点表示一个feature,叶结点表示一个类。

## 1.2 决策树 与 if-then
当使用决策树进行分类的时候，从根结点开始，对实例的某一个特征进行测试,根据测试的结果，将实例分配到子节点向下走，每一个子结点都对应这这个特征的一个取值，递归直到到达叶结点。每次测试的结果，要么是导出最终结论，要么是导出进一步判定的问题，进一步判定问题的考虑范围是在上次决策结果的限定范围之内。

决策树的路径或其所对应的if-then规则集合具有一个重要的性质: 互斥且完备，即,每个实例有且仅有一条路径对应。

问题: 如果测试数据集的某个特征出现了训练数据集中没有出现的取值怎么办。

## 2 决策树学习准备
决策树学习的本质上是从训练数据集中归纳出一组分类的规则, 与训练数据集不相矛盾的决策树(完全正确分类训练集)可能有很多，也可能没有。我们需要的是与训练数据集矛盾较小的决策树，注意，我们不仅仅希望我们的模型在训练集上的效果很好，还希望它对未知数据有很好的泛化能力。

## 2.1 特征选择
对一个样本来说，可能会有很多的特征，不同的特征对于我们决策的帮助也不同，这就涉及到一个特征选择的问题。如果根据一个特征进行分类的结果与随机分类的结果没有很大差别，则称这个特征是没有分类能力的，经验上，去除这样的特征对决策树的学习影响不大。

**特征选择的准则**: 信息增益/信息增益比

### 2.1.1 熵
**熵(entropy)**: 是表示随机变量不确定性的度量，假设x是一个取值空间容量有限的离散随机变量:
$$P(X=x_i) = p_i$$
则随机变量X的熵为:
$$H(X) = - \sum_{i=1}^n p_ilogp_i$$

信息熵有几个基本的性质:

1. 信息熵只依赖于X的分布，和X的取值无关
2. 熵越大，随机变量的不确定性就越大，考虑极端的情况 $0log0 = 0$ $1log1 = 0$
3. $0 \leq H(p) \leq log n$

### 2.1.2 条件熵 
**条件熵**: H(Y|X) 表示在已知随机变量X的条件下随机变量Y的不确定性,对$i=1,2,3,...n$, $j=1,2,3,...,m$
$$H(Y|X) = \sum_{i=1}^n p_iH(Y|X = x_i)$$
其中, $p_i = P(X = x_i), i=1,2,...,n$ 

### 2.1.3 经验熵与经验条件熵
当熵和条件熵中的概率(注意，我们虽然假设了分布的存在，但是这个分布实际是不可知的)，由估计得到(例如，极大似然估计)，我们分别称这时候的熵是经验熵和经验条件熵(empirical (conditional) entropy)

### 2.1.4 信息增益
**信息增益**: 特征A对训练数据集D的信息增益g(D,A)定义为集合D的经验熵H(D)与特征A给定条件下D的经验条件熵H(D|A)之差:(熵减)
$$g(D,A) = H(D) - H(D|A)$$


## 2.2 信息增益算法
假设对训练数据集D, |D|表示其样本容量，设有K个类$C_k, k = 1,2,...,K$, $|C_k|$是属于K类的样本个数，显然有$\sum_{k=1}^K|C_k| = |D|$. 

设特征A有n个不同的取值$\{a_1,a_2,...,a_n\}$,根据特征A的取值划分为n个子集,$D_1,D_2,...,D_n$,则有:$\sum_{i=1}^n|D_i| = |D|$

再设子集$D_i$中属于类k的样本集合为$D_{ik}$，其样本容量为$|D_{ik}|$,最终有:

*Algorithm*: Calculate information gain

input: training set D and feature A

output: information gain related to D & A: g(D,A)

- 1. 计算数据集D的经验熵$H(D) = -\sum^K_{k=1} \frac{|C_k|}{|D|}log_2 \frac{|C_k|}{|D|}$
- 2. 计算特征A对数据集D的经验条件熵$H(D|A) = -\sum_{i=1}^n \frac{|D_i|}{|D|}H(D_i) = -\sum_{i=1}^n \frac{|D_i|}{|D|}\sum_{k=1}^K \frac{|D_{ik}|}{|D_i|}log_2 \frac{|D_{ik}|}{|D_i|}$
- 3. g(D,A) = H(D) - H(D|A)

## 2.3 信息增益比
用信息增益作为划分训练数据集的特征，有一个问题是，偏向于选择取值较多的特征。为了缓解这个问题，引入信息增益比(information gain ratio).

**信息增益比**
特征A对训练数据集D的信息增益比$g_R(D,A)$定义为:
$$g_R(D,A) = \frac{g(D,A)}{H_A(D)}$$
其中$H_A(D) = - \sum_{i=1}^n \frac{|D_i|}{|D|} log_2 \frac{|D_i|}{|D|}$

需要注意的是，增益率准则对可取值数目较少的属性有所偏好，因此C4.5算法并不是直接选择增益率最大的候选划分属性，而是先从候选划分属性中找出信息增益高于平均水平的属性，然后从中选择增益率最高的。

## 3 决策树的生成算法
这部分介绍两种决策树的生成算法

## 3.1 ID3 Algorithm
input： training set D, feature set A, threshold $\epsilon$

output：Decision Tree T

- 1. 如果D中的所有instances 属于同一个类 $C_k$,则T为以$C_k$为类标记的单结点树，**返回T.**
- 2. 如果A为空集，即所有的可供决策的特征已经使用完全，则T为单结点树，使用多数表决的方式决定$C_k$类标记，**返回T**.
- 3. 否则，计算特征A中各个特征对D的信息增益，选择信息增益最大的特征$A_g$.
- 4. 如果$A_g$的信息增益小于$\epsilon$,则T为单结点树，使用多数表决的方式决定$C_k$类标记,**返回T**.
- 5. 否则，对$A_g$每一个可能的特征取值$a_i$分割数据集D，将$D_i$中实例数最大的类作为标记，构建子结点，由结点和子结点构成树T，返回T
- 6. 对第i个子结点,以$D_i$为训练集,$A - \{A_g\}$为特征集合，递归建树，得到子树$T_i$，返回$T_i$.

## 3.2 C4.5 Algorithm
将第三步的信息增益改换为信息增益比。

## 4 剪枝算法

剪枝(pruning)是决策树学习算法解决过拟合的主要手段，在决策树学习中，为了尽可能地正确分类训练样本，结点的划分可能太过复杂，决策树分支太多，导致把训练集自身的一些特点当作一般性质进行学习导致过拟合，因此可以通过主动去掉一些分支来降低过拟合的危险。

**(1) 预剪枝(prepruning)**

在决策树生成过程中，对每个结点在划分前先进行估计，如果当前结点的划分不能带来决策树泛化性能的提升，则停止划分并将当前结点标记为叶结点。

**(2) 后剪枝(postpruning)**

先从训练集生成一棵完整的决策树，然后自底向上地对非叶结点进行考察，如果将该结点对应的子树替换成叶结点能够带来决策树泛化性能的提升，则将该子树替换成叶结点。

显然，剪枝算法的核心问题是如何判断决策树的泛化性能提升。这里给出一个可能的方法：采用留出法，对数据集进行划分，预留一部分作为验证集来进行性能评估。

### 4.1 预剪枝

预剪枝算法使得决策树的很多分支不会展开，降低了过拟合的风险，同时节省了决策树训练时间和测试时间的开销，但是，有一些分支虽然当前划分不能提高泛化性能甚至导致泛化性能下降，但是后序的划分可能对性能提升很有帮助，基于"贪心算法"的预剪枝，会给决策树带来欠拟合的风险！

具体地说，在某一轮迭代中，我们是否要对此时的最优属性进行划分展开分支，我们这样来判断。

1. 采用多数表决的方式将这个结点定为叶结点，在测试集上得到准确率。
2. 对属性各个取值进行划分，得到几个分支，在这些分支的内部范围内，采用多数表决得到最终判断结果，再在测试集上得到准确率。
3. 比较两个准确率，选择准确率高的方式。

### 4.2 后剪枝

后剪枝算法是在得到一棵完整的决策树后，自底向上对所有非叶结点进行考察，如果把这个分支按多数表决换成叶节点后泛化性能提高，那么就把这个分支用叶结点替代。后剪枝得到的决策树通常比预剪枝决策树保留了更多的分支，一般情况下，后剪枝决策树的欠拟合比较小，泛化性能往往比预剪枝决策树好，但是开销更大。

## 5 连续与缺失值

(1) 连续值的可取值数目不再有限，因此需要采用连续属性离散化技术。 具体地说，给定样本集D，以及连续属性a，假设 a 在 D上出现了n个不同的有序取值$\{a^1, a^2, ..., a^n\}$ 基于划分点t可以把D分成两个子集 $D_t^+ = \{x | x.a > t\} \quad  D_t^- = \{x | x.a \leq t\}$。

bi-partition algorithm :

对相邻的属性取值 ai 与 a(i+1), 显然t在区间[ai, a(i+1)) 上取任意值进行划分结果是相同的(对训练集D来说)，因此对于连续属性a，我们考察包含n-1个元素的候选划分点集合：
$$T_a = \{\frac{a^i + a^{i+1}}{2} | 1 \leq i \leq n-1\}$$

基于一个评判准则，我们选择最合适的划分点：
$$Gain(D, a) = max_{t\in T_a} Gain(D,a,t) = max_{t \in T_a} Ent(D) - \sum_{\lambda \in \{-,+\}} \frac{|D_t^{\lambda}|}{|D|}Ent(D_t^{\lambda})$$

**需要注意的是，连续属性在之后仍然可以作为后代结点的划分属性。具体地说， a <= 0.381 不禁止 a <= 0.294 的使用。**

(2) 缺失值，样本的某些属性值可能缺失，这可能是因为测试成本、隐私保护等等原因，我们需要解决两个问题：
1. 如何在属性值缺失的情况下进行划分属性的选择?
2. 给定划分属性，如果样本在该属性上的取值缺失，如何对样本进行划分？
3. 