## K-nearest neighbor(KNN)
## 1.1 Introduction
KNN是一种基本的分类和回归方法，这里主要讨论分类问题中的KNN。

KNN是一种懒惰学习的方法，即，它没有显示的学习过程，相反，其基本思想是，给定一个训练数据集，其中的instance 的 label 都是已知的，当输入一个新的未知 instance，根据其最近邻的k个实例，通过一种方式(例如多数表决)来进行预测其类别。

1. k值选择
2. 距离度量
3. 分类决策规则

## 2.1 KNN
Input: 
1. $T = \{(x_1, y_1), (x_2, y_2),..., (x_N, y_N)\}$, 其中 $x_i$ 是第i个实例的特征向量, $y_i$ 是第i个实例的类别
2. 未知实例 x

Output: x 的类别

1. 根据给定的距离度量规则,在训练集T中找到与x最近邻的k个点($N_k(x)$)
2. 在$N_k(x)$中根据分类决策规则决定x的类别y:
$$y = arg max_{c_j} \sum_{x_i \in N_k(x)} I(y_i = c_j)$$

## 3.1 模型
KNN中，如果我们把训练集、距离度量、k值、决策规则确定之后，对于任何输入的待预测实例，其类别唯一确定。

事实上，以最近邻法(K=1)为例子，每个训练集中的实例附近，距离该点比其他点更近的区域叫做单元，每个训练集实例都有一个属于自己的单元，这些单元对整个特征空间做了一个划分，而新输入的实例的类别由它所落在的单元唯一确定。

## 3.2 距离度量
1. 欧式距离
2. $L_p$距离
3. Minkowski distance
4. Manhattan distance

不同的度量规则决定了空间的划分是不同的。

## 3.3 k值的选择
> 1. k值较小

预测结果对近邻的实例点将会非常敏感，容易受到训练集中噪声的影响,即，k值减小意味着整体模型的复杂，容易过拟合

> 2. k值较大

较大的K值能够减小学习的估计误差，缺点是学习的近似误差将会增大，与输入点不太相似的点也会对结果产生影响，极端的K=N情况将导致模型过于简单，完全忽略训练集中大量有效信息。

通常k值的选取一个较小的值，这个值通过交叉验证来决定。

## 3.4 决策规则
$$f: R^n \rightarrow \{c_1, c_2,...,c_k\}$$
误分类的概率是:
$$P(Y \neq f(X)) = 1 - P(Y = F(X))$$
对任意给定的待测实例x,有:
$$\frac{1}{k} \sum_{x_i \in N_k(x)} I(y_i \neq c_j) = 1 - \frac{1}{k} \sum_{x_i \in N_k(x)} I(y_i = c_j)$$
$\sum_{x_i \in N_k(x)} I(y_i = c_j)$ 最大化,等价于误分类率(经验风险)最小化,所以KNN中最常用的一种决策规则是多数表决。



